#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ JXCT
"""

import os
import sys
import subprocess
import json
import time
import argparse
from pathlib import Path
from typing import Dict, List, Any

class PerformanceTestRunner:
    def __init__(self, verbose: bool = False):
        self.project_root = Path(__file__).parent.parent
        self.test_dir = self.project_root / "test" / "performance"
        self.reports_dir = self.project_root / "test_reports"
        self.verbose = verbose

        # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –æ—Ç—á—ë—Ç–æ–≤
        self.reports_dir.mkdir(exist_ok=True)

    def log(self, message: str, level: str = "INFO"):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏"""
        timestamp = time.strftime("%H:%M:%S")
        print(f"[{timestamp}] {level}: {message}")

    def run_command(self, command: List[str], cwd: Path = None) -> tuple[int, str, str]:
        """–ó–∞–ø—É—Å–∫ –∫–æ–º–∞–Ω–¥—ã –∏ –≤–æ–∑–≤—Ä–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        try:
            if self.verbose:
                print(f"Running: {' '.join(command)}")

            result = subprocess.run(
                command,
                cwd=cwd or self.project_root,
                capture_output=not self.verbose,
                text=True,
                timeout=600  # 10 –º–∏–Ω—É—Ç —Ç–∞–π–º–∞—É—Ç
            )
            return result.returncode, result.stdout, result.stderr
        except subprocess.TimeoutExpired:
            return -1, "", f"Command timed out: {' '.join(command)}"
        except Exception as e:
            return -1, "", str(e)

    def compile_performance_tests(self) -> bool:
        """–ö–æ–º–ø–∏–ª—è—Ü–∏—è —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        self.log("–ö–æ–º–ø–∏–ª—è—Ü–∏—è —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ—Å—Ç–æ–≤
        test_file = self.test_dir / "test_performance.cpp"
        if not test_file.exists():
            self.log("–§–∞–π–ª —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω", "WARNING")
            return False

        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —Ç–µ—Å—Ç—ã
        compile_command = [
            "g++", "-std=c++17", "-O2", "-I../include",
            str(test_file),
            "-o", str(self.test_dir / "performance_test"),
            "-lunity"
        ]

        returncode, stdout, stderr = self.run_command(compile_command, self.test_dir)

        if returncode != 0:
            self.log(f"–û—à–∏–±–∫–∞ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏: {stderr}", "ERROR")
            return False

        self.log("–¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ")
        return True

    def run_performance_tests(self) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        self.log("–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")

        test_executable = self.test_dir / "performance_test"
        if not test_executable.exists():
            self.log("–ò—Å–ø–æ–ª–Ω—è–µ–º—ã–π —Ñ–∞–π–ª —Ç–µ—Å—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω", "ERROR")
            return {}

        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã
        returncode, stdout, stderr = self.run_command([str(test_executable)])

        if returncode != 0:
            self.log(f"–¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å: {stderr}", "ERROR")
            return {}

        # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = self.parse_performance_results(stdout)
        self.log("–¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã")

        return results

    def parse_performance_results(self, output: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        results = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "tests": {},
            "summary": {
                "total_tests": 0,
                "passed_tests": 0,
                "failed_tests": 0,
                "performance_metrics": {}
            }
        }

        lines = output.split('\n')
        current_test = None

        for line in lines:
            line = line.strip()

            # –ò—â–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤
            if "Performance:" in line:
                current_test = line.split("Performance:")[0].strip()
                results["tests"][current_test] = {
                    "status": "passed",
                    "metrics": {},
                    "details": []
                }
                results["summary"]["total_tests"] += 1

            # –ò—â–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            elif current_test and ":" in line and ("ms" in line or "per second" in line):
                try:
                    key, value = line.split(":", 1)
                    key = key.strip()
                    value = value.strip()

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    if "ms" in value:
                        numeric_value = float(value.split()[0])
                        results["tests"][current_test]["metrics"][key] = {
                            "value": numeric_value,
                            "unit": "ms"
                        }
                    elif "per second" in value:
                        numeric_value = float(value.split()[0])
                        results["tests"][current_test]["metrics"][key] = {
                            "value": numeric_value,
                            "unit": "ops/sec"
                        }

                except (ValueError, IndexError):
                    pass

            # –ò—â–µ–º –¥–µ—Ç–∞–ª–∏ —Ç–µ—Å—Ç–æ–≤
            elif current_test and line:
                results["tests"][current_test]["details"].append(line)

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for test_name, test_data in results["tests"].items():
            if test_data["status"] == "passed":
                results["summary"]["passed_tests"] += 1
            else:
                results["summary"]["failed_tests"] += 1

        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        all_metrics = []
        for test_data in results["tests"].values():
            for metric_name, metric_data in test_data["metrics"].items():
                if metric_data["unit"] == "ms":
                    all_metrics.append(metric_data["value"])

        if all_metrics:
            results["summary"]["performance_metrics"] = {
                "average_response_time": sum(all_metrics) / len(all_metrics),
                "min_response_time": min(all_metrics),
                "max_response_time": max(all_metrics),
                "total_operations": len(all_metrics)
            }

        return results

    def run_system_benchmarks(self) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤"""
        self.log("–ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤...")

        benchmarks = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "system_info": self.get_system_info(),
            "benchmarks": {}
        }

        # –¢–µ—Å—Ç CPU
        benchmarks["benchmarks"]["cpu"] = self.benchmark_cpu()

        # –¢–µ—Å—Ç –ø–∞–º—è—Ç–∏
        benchmarks["benchmarks"]["memory"] = self.benchmark_memory()

        # –¢–µ—Å—Ç –¥–∏—Å–∫–∞
        benchmarks["benchmarks"]["disk"] = self.benchmark_disk()

        self.log("–°–∏—Å—Ç–µ–º–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã")
        return benchmarks

    def get_system_info(self) -> Dict[str, str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∏—Å—Ç–µ–º–µ"""
        import platform

        return {
            "platform": platform.system(),
            "platform_version": platform.version(),
            "architecture": platform.machine(),
            "processor": platform.processor(),
            "python_version": platform.python_version()
        }

    def benchmark_cpu(self) -> Dict[str, float]:
        """–ë–µ–Ω—á–º–∞—Ä–∫ CPU"""
        import time

        # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç CPU
        start_time = time.time()

        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
        result = 0
        for i in range(1000000):
            result += i * i

        end_time = time.time()
        cpu_time = end_time - start_time

        return {
            "computation_time": cpu_time,
            "operations_per_second": 1000000 / cpu_time
        }

    def benchmark_memory(self) -> Dict[str, float]:
        """–ë–µ–Ω—á–º–∞—Ä–∫ –ø–∞–º—è—Ç–∏"""
        import time

        # –¢–µ—Å—Ç –∞–ª–ª–æ–∫–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
        start_time = time.time()

        # –°–æ–∑–¥–∞—ë–º –±–æ–ª—å—à–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
        large_list = [i for i in range(100000)]
        large_dict = {str(i): i for i in range(100000)}

        end_time = time.time()
        memory_time = end_time - start_time

        return {
            "allocation_time": memory_time,
            "list_size": len(large_list),
            "dict_size": len(large_dict)
        }

    def benchmark_disk(self) -> Dict[str, float]:
        """–ë–µ–Ω—á–º–∞—Ä–∫ –¥–∏—Å–∫–∞"""
        import time

        test_file = self.reports_dir / "disk_benchmark_test.tmp"

        # –¢–µ—Å—Ç –∑–∞–ø–∏—Å–∏
        start_time = time.time()
        with open(test_file, 'w') as f:
            for i in range(10000):
                f.write(f"Test line {i}\n")
        write_time = time.time() - start_time

        # –¢–µ—Å—Ç —á—Ç–µ–Ω–∏—è
        start_time = time.time()
        with open(test_file, 'r') as f:
            content = f.read()
        read_time = time.time() - start_time

        # –û—á–∏—Å—Ç–∫–∞
        test_file.unlink(missing_ok=True)

        return {
            "write_time": write_time,
            "read_time": read_time,
            "write_speed": 10000 / write_time,  # —Å—Ç—Ä–æ–∫ –≤ —Å–µ–∫—É–Ω–¥—É
            "read_speed": 10000 / read_time     # —Å—Ç—Ä–æ–∫ –≤ —Å–µ–∫—É–Ω–¥—É
        }

    def generate_performance_report(self, test_results: Dict[str, Any],
                                  benchmark_results: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        report_file = self.reports_dir / "performance-report.html"

        html_content = f"""
<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>–û—Ç—á—ë—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ JXCT</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background: #f0f0f0; padding: 20px; border-radius: 5px; }}
        .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
        .test {{ margin: 10px 0; padding: 10px; background: #f9f9f9; border-radius: 3px; }}
        .metric {{ display: inline-block; margin: 5px 10px; padding: 5px; background: #e8f4f8; border-radius: 3px; }}
        .passed {{ color: green; }}
        .failed {{ color: red; }}
        .warning {{ color: orange; }}
        table {{ border-collapse: collapse; width: 100%; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #f2f2f2; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>üöÄ –û—Ç—á—ë—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ JXCT</h1>
        <p><strong>–î–∞—Ç–∞:</strong> {test_results.get('timestamp', 'N/A')}</p>
        <p><strong>–°–∏—Å—Ç–µ–º–∞:</strong> {benchmark_results.get('system_info', {}).get('platform', 'N/A')} {benchmark_results.get('system_info', {}).get('architecture', 'N/A')}</p>
    </div>

    <div class="section">
        <h2>üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏</h2>
        <p><strong>–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤:</strong> {test_results.get('summary', {}).get('total_tests', 0)}</p>
        <p><strong>–ü—Ä–æ–π–¥–µ–Ω–æ:</strong> <span class="passed">{test_results.get('summary', {}).get('passed_tests', 0)}</span></p>
        <p><strong>–ü—Ä–æ–≤–∞–ª–µ–Ω–æ:</strong> <span class="failed">{test_results.get('summary', {}).get('failed_tests', 0)}</span></p>

        <h3>–î–µ—Ç–∞–ª–∏ —Ç–µ—Å—Ç–æ–≤:</h3>
"""

        for test_name, test_data in test_results.get('tests', {}).items():
            status_class = "passed" if test_data.get('status') == 'passed' else "failed"
            html_content += f"""
        <div class="test">
            <h4 class="{status_class}">‚úÖ {test_name}</h4>
"""

            for metric_name, metric_data in test_data.get('metrics', {}).items():
                html_content += f"""
            <div class="metric">
                <strong>{metric_name}:</strong> {metric_data['value']:.2f} {metric_data['unit']}
            </div>
"""

            html_content += """
        </div>
"""

        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏
        html_content += """
    </div>

    <div class="section">
        <h2>üíª –°–∏—Å—Ç–µ–º–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏</h2>
        <table>
            <tr>
                <th>–ú–µ—Ç—Ä–∏–∫–∞</th>
                <th>–ó–Ω–∞—á–µ–Ω–∏–µ</th>
                <th>–ï–¥–∏–Ω–∏—Ü–∞</th>
            </tr>
"""

        benchmarks = benchmark_results.get('benchmarks', {})

        # CPU –±–µ–Ω—á–º–∞—Ä–∫
        cpu_bench = benchmarks.get('cpu', {})
        html_content += f"""
            <tr>
                <td>CPU - –í—Ä–µ–º—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π</td>
                <td>{cpu_bench.get('computation_time', 0):.3f}</td>
                <td>—Å–µ–∫—É–Ω–¥—ã</td>
            </tr>
            <tr>
                <td>CPU - –û–ø–µ—Ä–∞—Ü–∏–π –≤ —Å–µ–∫—É–Ω–¥—É</td>
                <td>{cpu_bench.get('operations_per_second', 0):.0f}</td>
                <td>ops/sec</td>
            </tr>
"""

        # Memory –±–µ–Ω—á–º–∞—Ä–∫
        memory_bench = benchmarks.get('memory', {})
        html_content += f"""
            <tr>
                <td>Memory - –í—Ä–µ–º—è –∞–ª–ª–æ–∫–∞—Ü–∏–∏</td>
                <td>{memory_bench.get('allocation_time', 0):.3f}</td>
                <td>—Å–µ–∫—É–Ω–¥—ã</td>
            </tr>
            <tr>
                <td>Memory - –†–∞–∑–º–µ—Ä —Å–ø–∏—Å–∫–∞</td>
                <td>{memory_bench.get('list_size', 0):,}</td>
                <td>—ç–ª–µ–º–µ–Ω—Ç–æ–≤</td>
            </tr>
"""

        # Disk –±–µ–Ω—á–º–∞—Ä–∫
        disk_bench = benchmarks.get('disk', {})
        html_content += f"""
            <tr>
                <td>Disk - –°–∫–æ—Ä–æ—Å—Ç—å –∑–∞–ø–∏—Å–∏</td>
                <td>{disk_bench.get('write_speed', 0):.0f}</td>
                <td>—Å—Ç—Ä–æ–∫/—Å–µ–∫</td>
            </tr>
            <tr>
                <td>Disk - –°–∫–æ—Ä–æ—Å—Ç—å —á—Ç–µ–Ω–∏—è</td>
                <td>{disk_bench.get('read_speed', 0):.0f}</td>
                <td>—Å—Ç—Ä–æ–∫/—Å–µ–∫</td>
            </tr>
"""

        html_content += """
        </table>
    </div>

    <div class="section">
        <h2>üìà –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</h2>
        <ul>
            <li>–ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–µ–≥—É–ª—è—Ä–Ω–æ</li>
            <li>–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏</li>
            <li>–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è</li>
            <li>–ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏</li>
        </ul>
    </div>
</body>
</html>
"""

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(html_content)

        return str(report_file)

    def save_json_report(self, test_results: Dict[str, Any],
                        benchmark_results: Dict[str, Any]) -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON –æ—Ç—á—ë—Ç–∞"""
        combined_report = {
            "test_results": test_results,
            "benchmark_results": benchmark_results,
            "generated_at": time.strftime("%Y-%m-%d %H:%M:%S")
        }

        report_file = self.reports_dir / "performance-report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(combined_report, f, indent=2, ensure_ascii=False)

        return str(report_file)

    def run_all_tests(self) -> bool:
        """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        self.log("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")

        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —Ç–µ—Å—Ç—ã
        if not self.compile_performance_tests():
            self.log("‚ùå –û—à–∏–±–∫–∞ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤", "ERROR")
            return False

        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        test_results = self.run_performance_tests()
        if not test_results:
            self.log("‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏", "ERROR")
            return False

        # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏
        benchmark_results = self.run_system_benchmarks()

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á—ë—Ç—ã
        html_report = self.generate_performance_report(test_results, benchmark_results)
        json_report = self.save_json_report(test_results, benchmark_results)

        self.log(f"üìä HTML –æ—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {html_report}")
        self.log(f"üìä JSON –æ—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {json_report}")

        # –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É
        summary = test_results.get('summary', {})
        self.log(f"üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {summary.get('passed_tests', 0)}/{summary.get('total_tests', 0)} —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–π–¥–µ–Ω–æ")

        if summary.get('performance_metrics'):
            metrics = summary['performance_metrics']
            self.log(f"‚ö° –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {metrics.get('average_response_time', 0):.2f}ms")

        return True

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description="–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ JXCT")
    parser.add_argument("--verbose", "-v", action="store_true", help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥")
    parser.add_argument("--quick", "-q", action="store_true", help="–ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç (—Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏)")

    args = parser.parse_args()

    runner = PerformanceTestRunner(verbose=args.verbose)

    try:
        success = runner.run_all_tests()

        if success:
            print("\n‚úÖ –¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
            sys.exit(0)
        else:
            print("\n‚ùå –¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å!")
            sys.exit(1)

    except KeyboardInterrupt:
        print("\n\n‚ùå –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
