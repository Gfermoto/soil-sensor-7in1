#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ JXCT
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç PlatformIO native environment
"""

import os
import sys
import subprocess
import json
import time
import argparse
from pathlib import Path
from typing import Dict, List, Any

class PerformanceTestRunner:
    def __init__(self, verbose: bool = False):
        self.project_root = Path(__file__).parent.parent
        self.test_dir = self.project_root / "test" / "performance"
        self.reports_dir = self.project_root / "test_reports"
        self.verbose = verbose

        # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –æ—Ç—á—ë—Ç–æ–≤
        self.reports_dir.mkdir(exist_ok=True)

    def log(self, message: str, level: str = "INFO"):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏"""
        timestamp = time.strftime("%H:%M:%S")
        print(f"[{timestamp}] {level}: {message}")

    def run_command(self, command: List[str], cwd: Path = None) -> tuple[int, str, str]:
        """–ó–∞–ø—É—Å–∫ –∫–æ–º–∞–Ω–¥—ã –∏ –≤–æ–∑–≤—Ä–∞—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        try:
            if self.verbose:
                print(f"Running: {' '.join(command)}")

            result = subprocess.run(
                command,
                cwd=cwd or self.project_root,
                capture_output=not self.verbose,
                text=True,
                timeout=600  # 10 –º–∏–Ω—É—Ç —Ç–∞–π–º–∞—É—Ç
            )
            return result.returncode, result.stdout, result.stderr
        except subprocess.TimeoutExpired:
            return -1, "", f"Command timed out: {' '.join(command)}"
        except Exception as e:
            return -1, "", str(e)

    def compile_performance_tests(self) -> bool:
        """–ö–æ–º–ø–∏–ª—è—Ü–∏—è —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ PlatformIO"""
        self.log("–ö–æ–º–ø–∏–ª—è—Ü–∏—è —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ PlatformIO...")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ—Å—Ç–æ–≤
        test_file = self.test_dir / "test_performance.cpp"
        if not test_file.exists():
            self.log("–§–∞–π–ª —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω", "WARNING")
            return False

        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ PlatformIO native environment
        compile_command = [
            "pio", "test", "-e", "native", "-f", "performance"
        ]

        returncode, stdout, stderr = self.run_command(compile_command)

        if returncode != 0:
            self.log(f"–û—à–∏–±–∫–∞ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏ —á–µ—Ä–µ–∑ PlatformIO: {stderr}", "ERROR")
            return False

        self.log("–¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ —á–µ—Ä–µ–∑ PlatformIO")
        return True

    def run_performance_tests(self) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        self.log("–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")

        # –ó–∞–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ PlatformIO
        run_command = [
            "pio", "test", "-e", "native", "-f", "performance"
        ]

        returncode, stdout, stderr = self.run_command(run_command)

        if returncode != 0:
            self.log(f"–¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å: {stderr}", "ERROR")
            return {}

        # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = self.parse_performance_results(stdout)
        self.log("–¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã")

        return results

    def parse_performance_results(self, output: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        results = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "tests": {},
            "summary": {
                "total_tests": 0,
                "passed_tests": 0,
                "failed_tests": 0,
                "performance_metrics": {}
            }
        }

        lines = output.split('\n')
        current_test = None

        for line in lines:
            line = line.strip()

            # –ò—â–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤
            if "Performance:" in line:
                current_test = line.split("Performance:")[0].strip()
                results["tests"][current_test] = {
                    "status": "passed",
                    "metrics": {},
                    "details": []
                }
                results["summary"]["total_tests"] += 1

            # –ò—â–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            elif current_test and ":" in line and ("ms" in line or "per second" in line):
                try:
                    key, value = line.split(":", 1)
                    key = key.strip()
                    value = value.strip()

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    if "ms" in value:
                        numeric_value = float(value.split()[0])
                        results["tests"][current_test]["metrics"][key] = {
                            "value": numeric_value,
                            "unit": "ms"
                        }
                    elif "per second" in value:
                        numeric_value = float(value.split()[0])
                        results["tests"][current_test]["metrics"][key] = {
                            "value": numeric_value,
                            "unit": "ops/sec"
                        }

                except (ValueError, IndexError):
                    pass

            # –ò—â–µ–º –¥–µ—Ç–∞–ª–∏ —Ç–µ—Å—Ç–æ–≤
            elif current_test and line:
                results["tests"][current_test]["details"].append(line)

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for test_name, test_data in results["tests"].items():
            if test_data["status"] == "passed":
                results["summary"]["passed_tests"] += 1
            else:
                results["summary"]["failed_tests"] += 1

        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        all_metrics = []
        for test_data in results["tests"].values():
            for metric_name, metric_data in test_data["metrics"].items():
                if metric_data["unit"] == "ms":
                    all_metrics.append(metric_data["value"])

        if all_metrics:
            results["summary"]["performance_metrics"] = {
                "average_response_time": sum(all_metrics) / len(all_metrics),
                "min_response_time": min(all_metrics),
                "max_response_time": max(all_metrics),
                "total_operations": len(all_metrics)
            }

        return results

    def run_system_benchmarks(self) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤"""
        self.log("–ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤...")

        benchmarks = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "system_info": self.get_system_info(),
            "benchmarks": {}
        }

        # –¢–µ—Å—Ç CPU
        benchmarks["benchmarks"]["cpu"] = self.benchmark_cpu()

        # –¢–µ—Å—Ç –ø–∞–º—è—Ç–∏
        benchmarks["benchmarks"]["memory"] = self.benchmark_memory()

        # –¢–µ—Å—Ç –¥–∏—Å–∫–∞
        benchmarks["benchmarks"]["disk"] = self.benchmark_disk()

        self.log("–°–∏—Å—Ç–µ–º–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã")
        return benchmarks

    def get_system_info(self) -> Dict[str, str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∏—Å—Ç–µ–º–µ"""
        import platform

        return {
            "platform": platform.system(),
            "version": platform.version(),
            "architecture": platform.machine(),
            "processor": platform.processor(),
            "python_version": platform.python_version()
        }

    def benchmark_cpu(self) -> Dict[str, float]:
        """–ë–µ–Ω—á–º–∞—Ä–∫ CPU"""
        import time

        start_time = time.time()

        # –ü—Ä–æ—Å—Ç–æ–π CPU —Ç–µ—Å—Ç - –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ—Ä–∏–∞–ª–∞
        def factorial(n):
            if n <= 1:
                return 1
            return n * factorial(n - 1)

        # –í—ã—á–∏—Å–ª—è–µ–º —Ñ–∞–∫—Ç–æ—Ä–∏–∞–ª 1000 —Ä–∞–∑
        for _ in range(1000):
            factorial(10)

        end_time = time.time()
        cpu_time = (end_time - start_time) * 1000  # –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö

        return {
            "cpu_test_time_ms": cpu_time,
            "operations_per_second": 1000.0 / (cpu_time / 1000.0)
        }

    def benchmark_memory(self) -> Dict[str, float]:
        """–ë–µ–Ω—á–º–∞—Ä–∫ –ø–∞–º—è—Ç–∏"""
        import time

        start_time = time.time()

        # –¢–µ—Å—Ç –∞–ª–ª–æ–∫–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏
        test_data = []
        for i in range(10000):
            test_data.append([i] * 100)

        allocation_time = (time.time() - start_time) * 1000

        # –¢–µ—Å—Ç –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
        start_time = time.time()
        test_data.clear()
        deallocation_time = (time.time() - start_time) * 1000

        return {
            "allocation_time_ms": allocation_time,
            "deallocation_time_ms": deallocation_time,
            "total_memory_operations": 10000
        }

    def benchmark_disk(self) -> Dict[str, float]:
        """–ë–µ–Ω—á–º–∞—Ä–∫ –¥–∏—Å–∫–∞"""
        import time

        test_file = self.reports_dir / "disk_benchmark_test.tmp"

        # –¢–µ—Å—Ç –∑–∞–ø–∏—Å–∏
        start_time = time.time()
        with open(test_file, 'w') as f:
            for i in range(1000):
                f.write(f"Test line {i}: " + "x" * 100 + "\n")
        write_time = (time.time() - start_time) * 1000

        # –¢–µ—Å—Ç —á—Ç–µ–Ω–∏—è
        start_time = time.time()
        with open(test_file, 'r') as f:
            content = f.read()
        read_time = (time.time() - start_time) * 1000

        # –û—á–∏—Å—Ç–∫–∞
        test_file.unlink(missing_ok=True)

        return {
            "write_time_ms": write_time,
            "read_time_ms": read_time,
            "file_size_bytes": len(content)
        }

    def generate_performance_report(self, test_results: Dict[str, Any],
                                  benchmark_results: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        report = []
        report.append("# üìä –û—Ç—á—ë—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ JXCT")
        report.append("")
        report.append(f"**–î–∞—Ç–∞**: {test_results.get('timestamp', 'N/A')}")
        report.append(f"**–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞**: {benchmark_results.get('system_info', {}).get('platform', 'N/A')}")
        report.append("")

        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤
        report.append("## üß™ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
        report.append("")

        if test_results.get('tests'):
            for test_name, test_data in test_results['tests'].items():
                status_emoji = "‚úÖ" if test_data['status'] == 'passed' else "‚ùå"
                report.append(f"### {status_emoji} {test_name}")

                if test_data.get('metrics'):
                    for metric_name, metric_data in test_data['metrics'].items():
                        report.append(f"- **{metric_name}**: {metric_data['value']} {metric_data['unit']}")

                if test_data.get('details'):
                    for detail in test_data['details'][:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3 –¥–µ—Ç–∞–ª–∏
                        report.append(f"  - {detail}")

                report.append("")
        else:
            report.append("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ —Ç–µ—Å—Ç–∞—Ö")
            report.append("")

        # –°–∏—Å—Ç–µ–º–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏
        report.append("## üíª –°–∏—Å—Ç–µ–º–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏")
        report.append("")

        if benchmark_results.get('benchmarks'):
            for benchmark_name, benchmark_data in benchmark_results['benchmarks'].items():
                report.append(f"### {benchmark_name.upper()}")
                for metric_name, metric_value in benchmark_data.items():
                    if isinstance(metric_value, float):
                        report.append(f"- **{metric_name}**: {metric_value:.2f}")
                    else:
                        report.append(f"- **{metric_name}**: {metric_value}")
                report.append("")

        # –°–≤–æ–¥–∫–∞
        summary = test_results.get('summary', {})
        report.append("## üìà –°–≤–æ–¥–∫–∞")
        report.append("")
        report.append(f"- **–í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤**: {summary.get('total_tests', 0)}")
        report.append(f"- **–ü—Ä–æ–π–¥–µ–Ω–æ**: {summary.get('passed_tests', 0)}")
        report.append(f"- **–ü—Ä–æ–≤–∞–ª–µ–Ω–æ**: {summary.get('failed_tests', 0)}")

        if summary.get('performance_metrics'):
            metrics = summary['performance_metrics']
            report.append(f"- **–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞**: {metrics.get('average_response_time', 0):.2f} ms")
            report.append(f"- **–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è**: {metrics.get('min_response_time', 0):.2f} ms")
            report.append(f"- **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è**: {metrics.get('max_response_time', 0):.2f} ms")

        return "\n".join(report)

    def save_json_report(self, test_results: Dict[str, Any],
                        benchmark_results: Dict[str, Any]) -> str:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON –æ—Ç—á—ë—Ç–∞"""
        combined_report = {
            "test_results": test_results,
            "benchmark_results": benchmark_results,
            "generated_at": time.strftime("%Y-%m-%d %H:%M:%S")
        }

        report_file = self.reports_dir / "performance-test-report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(combined_report, f, indent=2, ensure_ascii=False)

        return str(report_file)

    def run_all_tests(self) -> bool:
        """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        self.log("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")

        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —Ç–µ—Å—Ç—ã
        if not self.compile_performance_tests():
            self.log("‚ùå –¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å!", "ERROR")
            return False

        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã
        test_results = self.run_performance_tests()
        if not test_results:
            self.log("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤", "ERROR")
            return False

        # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏
        benchmark_results = self.run_system_benchmarks()

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á—ë—Ç—ã
        report_content = self.generate_performance_report(test_results, benchmark_results)
        report_file = self.reports_dir / "performance-report.md"

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

        json_file = self.save_json_report(test_results, benchmark_results)

        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        print("\n" + "="*60)
        print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò")
        print("="*60)

        summary = test_results.get('summary', {})
        total_tests = summary.get('total_tests', 0)
        passed_tests = summary.get('passed_tests', 0)

        if total_tests > 0:
            success_rate = (passed_tests / total_tests) * 100
            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1f}% ({passed_tests}/{total_tests})")
        else:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ —Ç–µ—Å—Ç–∞—Ö")

        print(f"üìÑ –û—Ç—á—ë—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"   - Markdown: {report_file}")
        print(f"   - JSON: {json_file}")

        return passed_tests == total_tests

def main():
    parser = argparse.ArgumentParser(description="–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ JXCT")
    parser.add_argument("-v", "--verbose", action="store_true", help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥")
    args = parser.parse_args()

    runner = PerformanceTestRunner(verbose=args.verbose)
        success = runner.run_all_tests()

    if not success:
        sys.exit(1)

if __name__ == "__main__":
    main()
