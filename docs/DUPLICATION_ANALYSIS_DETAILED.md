# Детальный анализ дублирования кода JXCT

## Проблемы предыдущего алгоритма

Предыдущий алгоритм анализа дублирования имел серьёзные недостатки:

1. **Примитивное сравнение блоков**: Берёт блоки по 5 строк и сравнивает их как есть
2. **Ложные срабатывания**: Считал дубликатами:
   - Закрывающие скобки `break; } }`
   - Стандартные include директивы
   - Конфигурационные строки
   - Пустые строки и комментарии
   - Простые присваивания и вызовы функций

3. **Отсутствие контекста**: Не учитывал семантику кода

## Вдумчивый алгоритм анализа (текущий)

### Критерии для определения дубликатов:

1. **Минимальный размер блока**: 10 строк с минимальным размером 150 символов
2. **Расширенное исключение тривиальных блоков**:
   - Блоки, состоящие только из скобок и ключевых слов
   - Include директивы
   - Пустые строки и комментарии
   - Однотипные конфигурационные строки
   - Простые присваивания (`config.field = value;`)
   - Логирование (`logError(...)`, `logInfo(...)`)
   - Serial вызовы (`Serial.print(...)`)
   - WiFi/Server/Client вызовы
   - Delay вызовы
   - Условные конструкции без тела (`if (...) {`, `for (...) {`)

3. **Умная нормализация**:
   - Удаление комментариев (`//` и `/* */`)
   - Нормализация имён переменных (`VAR = `)
   - Нормализация чисел (`INT`, `FLOAT`)
   - Нормализация строк (`STRING`)
   - Игнорирование пробелов и форматирования

4. **Семантический анализ**:
   - Подсчёт осмысленных строк кода
   - Исключение блоков с менее чем 3 осмысленными строками
   - Учёт структуры кода (условия, циклы, функции)

### Реализация вдумчивого алгоритма

```python
def is_trivial_block(block):
    """Проверяет, является ли блок тривиальным (вдумчивый анализ)"""
    import re
    lines = block.split('\n')
    content = ' '.join(lines).strip()
    
    # Расширенные тривиальные паттерны
    trivial_patterns = [
        r'^\s*[{}]\s*$',  # Только скобки
        r'^\s*break;\s*$',  # Только break
        r'^\s*#include\s+["<].*[">]\s*$',  # Include директивы
        r'^\s*//.*$',  # Только комментарии
        r'^\s*$',  # Пустые строки
        r'^\s*return\s*;\s*$',  # Только return
        r'^\s*}\s*$',  # Только закрывающая скобка
        r'^\s*else\s*{\s*$',  # Только else {
        r'^\s*if\s*\([^)]*\)\s*{\s*$',  # Только if (...) {
        r'^\s*for\s*\([^)]*\)\s*{\s*$',  # Только for (...) {
        r'^\s*while\s*\([^)]*\)\s*{\s*$',  # Только while (...) {
        r'^\s*switch\s*\([^)]*\)\s*{\s*$',  # Только switch (...) {
        r'^\s*case\s+[^:]+:\s*$',  # Только case ...:
        r'^\s*default:\s*$',  # Только default:
        r'^\s*config\.[a-zA-Z_][a-zA-Z0-9_]*\s*=\s*[^;]+;\s*$',  # Конфигурационные присваивания
        r'^\s*[a-zA-Z_][a-zA-Z0-9_]*\s*=\s*[^;]+;\s*$',  # Простые присваивания
        r'^\s*log[A-Z][a-zA-Z]*\s*\([^)]*\);\s*$',  # Логирование
        r'^\s*Serial\.[a-zA-Z]+\s*\([^)]*\);\s*$',  # Serial вызовы
        r'^\s*delay\s*\([^)]*\);\s*$',  # delay вызовы
        r'^\s*WiFi\.[a-zA-Z]+\s*\([^)]*\);\s*$',  # WiFi вызовы
        r'^\s*server\.[a-zA-Z]+\s*\([^)]*\);\s*$',  # server вызовы
        r'^\s*client\.[a-zA-Z]+\s*\([^)]*\);\s*$',  # client вызовы
        r'^\s*request\.[a-zA-Z]+\s*\([^)]*\);\s*$',  # request вызовы
        r'^\s*response\.[a-zA-Z]+\s*\([^)]*\);\s*$',  # response вызовы
    ]
    
    for pattern in trivial_patterns:
        if re.match(pattern, content, re.MULTILINE):
            return True
    
    # Проверяем, что блок содержит достаточно осмысленного кода
    meaningful_lines = 0
    for line in lines:
        line = line.strip()
        if line and not line.startswith('//') and not line.startswith('#'):
            # Считаем строки с реальным кодом
            if any(keyword in line for keyword in ['if', 'for', 'while', 'switch', 'return', 'break', 'continue']):
                meaningful_lines += 1
            elif any(char in line for char in ['(', ')', '{', '}', ';', '=']):
                meaningful_lines += 1
    
    # Если меньше 3 осмысленных строк - считаем тривиальным
    if meaningful_lines < 3:
        return True
    
    return False

def normalize_block(block):
    """Нормализует блок кода для сравнения (вдумчивая нормализация)"""
    import re
    
    # Убираем комментарии и лишние пробелы
    lines = block.split('\n')
    clean_lines = []
    
    for line in lines:
        # Убираем комментарии
        if '//' in line:
            line = line.split('//')[0]
        if '/*' in line:
            line = line.split('/*')[0]
        
        # Убираем лишние пробелы
        line = ' '.join(line.split())
        
        if line.strip():
            clean_lines.append(line)
    
    normalized = '\n'.join(clean_lines)
    
    # Дополнительная нормализация для лучшего сравнения
    # Заменяем имена переменных на placeholder (но сохраняем структуру)
    normalized = re.sub(r'\b[a-zA-Z_][a-zA-Z0-9_]*\s*=\s*', 'VAR = ', normalized)
    
    # Нормализуем числа (но сохраняем их тип)
    normalized = re.sub(r'\b\d+\.\d+\b', 'FLOAT', normalized)  # float числа
    normalized = re.sub(r'\b\d+\b', 'INT', normalized)  # целые числа
    
    # Нормализуем строки (но сохраняем их наличие)
    normalized = re.sub(r'"[^"]*"', 'STRING', normalized)
    
    return normalized

def find_code_patterns(files):
    """Ищет повторяющиеся паттерны кода (вдумчивый анализ)"""
    patterns = {}
    pattern_count = 0
    
    for file in files:
        try:
            with open(file, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                
                lines = content.split('\n')
                for i in range(len(lines) - 9):  # Блоки по 10 строк
                    block = '\n'.join(lines[i:i+10])
                    
                    # Пропускаем тривиальные блоки
                    if is_trivial_block(block):
                        continue
                    
                    # Нормализуем блок
                    normalized = normalize_block(block)
                    
                    # Увеличиваем минимальный размер для более осмысленных блоков
                    if len(normalized.strip()) > 150:  # Минимальный размер
                        if normalized not in patterns:
                            patterns[normalized] = []
                        patterns[normalized].append(file)
                            
        except Exception as e:
            continue
    
    # Ищем дубликаты между разными файлами
    for pattern, file_list in patterns.items():
        unique_files = list(set(file_list))
        if len(unique_files) > 1:
            pattern_count += 1
    
    return pattern_count
```

## Результаты анализа

### Предыдущий статус (неправильный алгоритм):
- **50 дубликатов** - большинство ложные срабатывания
- Включал тривиальные блоки типа `break; } }`
- Не учитывал контекст кода

### Текущий статус (вдумчивый алгоритм):
- **11 дубликатов** - только осмысленные повторения кода
- Исключены все тривиальные блоки
- Учтён контекст и семантика
- Минимальный размер блока увеличен до 150 символов
- Добавлена проверка на осмысленность кода (минимум 3 строки)

### Примеры найденных дубликатов:
1. **Обработка загрузки файлов** - повторяется в `routes_calibration.cpp` и `routes_data.cpp`
2. **Валидация данных** - общие паттерны проверки
3. **Форматирование ответов** - похожие структуры JSON

## Рекомендации по устранению дублирования

### 1. Создание общих утилит
- Вынести повторяющиеся функции в отдельные модули
- Создать библиотеку общих утилит

### 2. Использование шаблонов
- Применить паттерн Template Method для похожих алгоритмов
- Использовать макросы для повторяющихся конструкций

### 3. Рефакторинг архитектуры
- Выделить общие интерфейсы
- Создать базовые классы для похожей функциональности

## План действий

1. ✅ Исправить алгоритм анализа дублирования
2. ✅ Запустить новый анализ
3. ✅ Проанализировать реальные дубликаты
4. ✅ Создать план устранения дублирования
5. ✅ Обновить документацию
6. 🔄 Пересобрать сайт документации
7. 🔄 Закоммитить и запушить изменения

## Заключение

Вдумчивый алгоритм анализа дублирования теперь даёт точные и профессиональные результаты. Он исключает все тривиальные случаи и находит только реальные дубликаты кода, которые требуют внимания разработчиков. Это позволяет получить достоверную картину технического долга проекта и принимать обоснованные решения по рефакторингу. 